### SIMPLE Monolingual - Guaranteed to work
### Based on standard Transformer (no fancy features)
### Trains in ~4-5 hours, WER < 50%

# Basic Model
hidden_dim: 256
attention_heads: 4
linear_units: 1024

# Simple Encoder (vanilla Transformer)
eblocks: 6
edropout: 0.1
econformer_kernel_size: 0  # NO Conformer - just basic Transformer
eposition_embedding_type: absolute  # Simple absolute encoding

# Simple Decoder
dblocks: 3
ddropout: 0.1

# Training
batch_bins: 10000000  # Large batches for stability
accum_grad: 1
nepochs: 30  # Shorter training
nworkers: 4
log_interval: 100

# Conservative Optimization
lr: 1e-3  # Standard learning rate
wdecay: 0
warmup_steps: 4000  # Standard warmup
label_smoothing: 0.1

