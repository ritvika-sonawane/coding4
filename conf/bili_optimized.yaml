### Optimized config for Bilingual (English + Italian) - Checkpoints 3 & 4
### Target: WER < 30%, ACC > 90%

# Model Architecture - Larger model for better bilingual performance
hidden_dim: 512  # Doubled for more capacity
attention_heads: 8  # More attention heads
linear_units: 2048

# Encoder - Deeper for better representation
eblocks: 16  # More layers for complex bilingual patterns
edropout: 0.15  # Slightly higher dropout to prevent overfitting
econformer_kernel_size: 31  # Conformer convolutions
eposition_embedding_type: relative  # Better for long sequences

# Decoder
dblocks: 6
ddropout: 0.15

# Training
batch_bins: 3000000  # Reduced due to larger model
accum_grad: 4  # More accumulation for stable training
nepochs: 80  # More epochs for convergence
nworkers: 4
log_interval: 100

# Optimization - More careful for bilingual
lr: 1.5e-3
wdecay: 1e-5  # Slightly higher weight decay
warmup_steps: 20000  # Longer warmup for stability
label_smoothing: 0.1

