# Optimized Monolingual Configuration - Version 2
# Focuses on fast convergence and reduced training time

# Model Architecture - Slightly smaller for speed
hidden_dim: 256
attention_heads: 4
linear_units: 1024

# Encoder - Fewer blocks for speed, keep Conformer
eblocks: 8  # Reduced from 12
edropout: 0.1
econformer_kernel_size: 15  # Smaller kernel for speed
eposition_embedding_type: relative

# Decoder - Fewer blocks
dblocks: 4  # Reduced from 6
ddropout: 0.1

# Training - Faster batching
batch_bins: 6000000  # Increased from 4M for faster epochs
accum_grad: 1
nepochs: 40  # Reduced from 60
nworkers: 4
log_interval: 200

# Optimizer - Better schedule for faster convergence
lr: 2e-3  # Higher starting LR (was 1e-3)
wdecay: 1e-6
warmup_steps: 8000  # Reduced from 15000
label_smoothing: 0.1

