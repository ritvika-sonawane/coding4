### Fast Monolingual Config - Trains in ~3-4 hours
### Lighter model but should still achieve WER < 60%

# Smaller Model Architecture
hidden_dim: 128  # Reduced from 256
attention_heads: 4
linear_units: 512  # Reduced from 2048

# Encoder - Fewer layers
eblocks: 6  # Reduced from 12
edropout: 0.1
econformer_kernel_size: 0  # Disable Conformer for speed (vanilla Transformer)
eposition_embedding_type: absolute  # Simpler than relative

# Decoder
dblocks: 3  # Reduced from 6
ddropout: 0.1

# Training - Faster convergence
batch_bins: 8000000  # Larger batches
accum_grad: 1
nepochs: 40  # Reduced from 60
nworkers: 4
log_interval: 50

# Optimization - More aggressive
lr: 3e-3  # Higher learning rate
wdecay: 1e-6
warmup_steps: 5000  # Faster warmup
label_smoothing: 0.1

